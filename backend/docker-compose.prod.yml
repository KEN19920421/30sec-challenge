version: "3.9"

# =============================================================================
# 30sec Challenge — Production Docker Compose
# =============================================================================
# Usage:
#   docker compose -f docker-compose.prod.yml up -d --build
#
# Prerequisites:
#   - Copy .env.example to .env and fill in production values
#   - No MinIO service — production uses Cloudflare R2 or AWS S3 directly
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Application
  # ---------------------------------------------------------------------------
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: vc_app
    restart: unless-stopped
    ports:
      - "${PORT:-3000}:3000"
    env_file:
      - .env
    environment:
      NODE_ENV: production
      DATABASE_URL: postgres://${POSTGRES_USER:-app_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-video_challenge}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
        reservations:
          memory: 256M
          cpus: "0.25"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  # ---------------------------------------------------------------------------
  # PostgreSQL
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: vc_postgres
    restart: unless-stopped
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-app_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
      POSTGRES_DB: ${POSTGRES_DB:-video_challenge}
      # Locale for proper text sorting
      POSTGRES_INITDB_ARGS: "--locale=en_US.UTF-8"
    volumes:
      - pg_data:/var/lib/postgresql/data
    networks:
      - app_network
    # Production-tuned PostgreSQL settings
    command: >
      postgres
        -c shared_buffers=256MB
        -c effective_cache_size=768MB
        -c work_mem=8MB
        -c maintenance_work_mem=128MB
        -c wal_buffers=16MB
        -c max_connections=100
        -c checkpoint_completion_target=0.9
        -c random_page_cost=1.1
        -c effective_io_concurrency=200
        -c min_wal_size=1GB
        -c max_wal_size=4GB
        -c log_min_duration_statement=500
        -c log_checkpoints=on
        -c log_connections=on
        -c log_disconnections=on
        -c log_lock_waits=on
        -c log_temp_files=0
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-app_user} -d ${POSTGRES_DB:-video_challenge}"]
      interval: 10s
      timeout: 5s
      start_period: 30s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "2.0"
        reservations:
          memory: 512M
          cpus: "0.5"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ---------------------------------------------------------------------------
  # Redis
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: vc_redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    # Password-protected, AOF persistence, sensible memory limits
    command: >
      redis-server
        --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD is required}
        --appendonly yes
        --appendfsync everysec
        --maxmemory 512mb
        --maxmemory-policy allkeys-lru
        --tcp-backlog 511
        --timeout 300
        --tcp-keepalive 60
        --save 900 1
        --save 300 10
        --save 60 10000
    volumes:
      - redis_data:/data
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      start_period: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 640M
          cpus: "1.0"
        reservations:
          memory: 128M
          cpus: "0.25"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

# =============================================================================
# Volumes — persistent storage across container restarts
# =============================================================================
volumes:
  pg_data:
    driver: local
  redis_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  app_network:
    driver: bridge
